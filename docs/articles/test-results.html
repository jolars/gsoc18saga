<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>GSoC 2018 SAGA project test results • gsoc18saga</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">gsoc18saga</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/test-results.html">GSoC 2018 SAGA project test results</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>GSoC 2018 SAGA project test results</h1>
                        <h4 class="author">Johan Larsson</h4>
            
            <h4 class="date">2018-03-05</h4>
          </div>

    
    
<div class="contents">
<div id="easy" class="section level2">
<h2 class="hasAnchor">
<a href="#easy" class="anchor"></a>Easy</h2>
<p>Our first task is to fit a L1-regularized linear model to the spam data set from <strong>ElemStatLearn</strong> and analyze the results in terms of the selected features as well as test error and AUC. We will also compare our model to a naive model that predicts the most frequent class.</p>
<p>We begin by loading our libraries.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gsoc18saga)
<span class="kw">library</span>(glmnet)
<span class="kw">library</span>(ElemStatLearn)</code></pre></div>
<p>We will use the <code><a href="http://www.rdocumentation.org/packages/ElemStatLearn/topics/spam">ElemStatLearn::spam</a></code> dataset to try to classify email as either spam or regular email. We load the data and set up balanced train and test partitions using the <strong>caret</strong> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># extract the necessary data</span>
x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(spam[, <span class="op">-</span><span class="kw">ncol</span>(spam)])
y &lt;-<span class="st"> </span>spam<span class="op">$</span>spam
n &lt;-<span class="st"> </span><span class="kw">nrow</span>(x)

<span class="co"># create train and test sets</span>
train_id &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/caret/topics/createDataPartition">createDataPartition</a></span>(y, <span class="dt">p =</span> <span class="fl">0.75</span>)[[1L]]
train_x &lt;-<span class="st"> </span>x[train_id, ]
train_y &lt;-<span class="st"> </span>y[train_id]
test_x &lt;-<span class="st"> </span>x[<span class="op">-</span>train_id, ]
test_y &lt;-<span class="st"> </span>y[<span class="op">-</span>train_id]

<span class="co"># train the model</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)
fit &lt;-<span class="st"> </span>glmnet<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/glmnet/topics/cv.glmnet">cv.glmnet</a></span>(train_x, train_y, <span class="dt">family =</span> <span class="st">"binomial"</span>)</code></pre></div>
<p>The model’s chosen features are</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(<span class="kw">coef</span>(fit)[<span class="kw">coef</span>(fit)[, <span class="dv">1</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>, ])
<span class="co">#&gt;  [1] "A.3"  "A.4"  "A.5"  "A.6"  "A.7"  "A.8"  "A.9"  "A.10" "A.14" "A.15"</span>
<span class="co">#&gt; [11] "A.16" "A.17" "A.18" "A.19" "A.20" "A.21" "A.22" "A.23" "A.24" "A.28"</span>
<span class="co">#&gt; [21] "A.36" "A.52" "A.53" "A.54" "A.56" "A.57"</span></code></pre></div>
<p>Next we’ll examine the performance of the model using Receiver Operating Characteristics (ROC), Area Under the Curve (AUC), and test error. We’ll compare the model against a naive classification scheme wherein we predict each observation (email) to be the most prevalent category of the training set, namely. <code>email</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pROC)

roc_glmnet &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/pROC/topics/roc">roc</a></span>(test_y,
                  <span class="kw">as.vector</span>(<span class="kw">predict</span>(fit, test_x, <span class="dt">type =</span> <span class="st">"response"</span>)),
                  <span class="dt">ci =</span> <span class="ot">TRUE</span>)
roc_naive &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/pROC/topics/roc">roc</a></span>(test_y, <span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">length</span>(test_y)), <span class="dt">ci =</span> <span class="ot">TRUE</span>)

<span class="kw">plot</span>(roc_glmnet, <span class="dt">print.auc =</span> <span class="ot">TRUE</span>)</code></pre></div>
<div class="figure">
<img src="test-results_files/figure-html/roc-1.png" alt="Receiver operating characteristic curves for the lasso model." width="384"><p class="caption">
Receiver operating characteristic curves for the lasso model.
</p>
</div>
<p>The 95% confidence level for the AUC value for our lasso model is <span class="math inline">\([0.9705949, 0.9853183]\)</span>. We can compare the glmnet model to the naive model using DeLong’s test</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="http://www.rdocumentation.org/packages/pROC/topics/roc.test">roc.test</a></span>(roc_glmnet, roc_naive)
<span class="co">#&gt; </span>
<span class="co">#&gt;  DeLong's test for two correlated ROC curves</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; data:  roc_glmnet and roc_naive</span>
<span class="co">#&gt; Z = 127.25, p-value &lt; 2.2e-16</span>
<span class="co">#&gt; alternative hypothesis: true difference in AUC is not equal to 0</span>
<span class="co">#&gt; sample estimates:</span>
<span class="co">#&gt; AUC of roc1 AUC of roc2 </span>
<span class="co">#&gt;   0.9779566   0.5000000</span></code></pre></div>
<p>which unsurprisingly shows that the glmnet model is significantly better. Looking at the accuracy of the model, the naive model has an accuracy of 0.606087 whilst the lasso model manages an accuracy of 0.9286957.</p>
</div>
<div id="medium" class="section level2">
<h2 class="hasAnchor">
<a href="#medium" class="anchor"></a>Medium</h2>
<p>In this part we are going to compare the performance of <strong>glmnet</strong> with that of <strong>bigoptim</strong> for L2-regularized model fits. For this purpose, we are going to use the <code>bigoptim::covtype.svm</code> data set and and see how timings differ depending on the numbers of columns and rows we use from the data. We will also study objective function values in the same fashion.</p>
<p>The results for these simulations have been precomputed through code that is available at <a href="https://github.com/jolars/gsoc18saga/blob/master/data-raw/benchmarks.R" class="uri">https://github.com/jolars/gsoc18saga/blob/master/data-raw/benchmarks.R</a>. The dataset is made part of this package and available as <code><a href="http://www.rdocumentation.org/packages/gsoc18saga/topics/data_medium">gsoc18saga::data_medium</a></code>.</p>
<p>Looking at the results from the simulation, we note that <code><a href="http://www.rdocumentation.org/packages/glmnet/topics/glmnet">glmnet()</a></code> and <code>sag_fit()</code> (bigoptim) perform on par with one another when either the rows or columns are relatively few. As we add more observations (rows) and covariates (columns), however, the SAG algorithm from <strong>bigoptim</strong> outperforms <code><a href="http://www.rdocumentation.org/packages/glmnet/topics/glmnet">glmnet()</a></code> in speed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gradpal &lt;-<span class="st"> </span><span class="kw">colorRampPalette</span>(RColorBrewer<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/RColorBrewer/topics/ColorBrewer">brewer.pal</a></span>(<span class="dv">11</span>, <span class="st">"RdBu"</span>))(<span class="dv">100</span>)

<span class="kw">library</span>(tidyr)
<span class="kw">library</span>(dplyr)

data_medium_diff &lt;-<span class="st"> </span>data_medium <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/tidyr/topics/gather">gather</a></span>(<span class="st">"var"</span>, <span class="st">"value"</span>, time, loss) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/tidyr/topics/spread">spread</a></span>(package, value) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span>(<span class="dt">diff =</span> glmnet <span class="op">-</span><span class="st"> </span>sag) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/select.html">select</a></span>(<span class="op">-</span>glmnet, <span class="op">-</span>sag) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/tidyr/topics/spread">spread</a></span>(var, diff)

z &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(data_medium_diff<span class="op">$</span>time))

lattice<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/lattice/topics/levelplot">levelplot</a></span>(
  time <span class="op">~</span><span class="st"> </span>cols<span class="op">*</span>rows, 
  <span class="dt">data =</span> data_medium_diff,
  <span class="dt">asp =</span> <span class="dv">1</span>,
  <span class="dt">at =</span> <span class="kw">seq</span>(<span class="op">-</span>z, z, <span class="dt">length.out =</span> <span class="dv">20</span>),
  <span class="dt">col.regions =</span> gradpal,
  <span class="dt">main =</span> <span class="st">"Speed (glmnet - bigoptim)"</span>
)</code></pre></div>
<div class="figure">
<img src="test-results_files/figure-html/unnamed-chunk-1-1.png" alt="Speed in microseconds of glmnet and bigoptim runs. Positive values (blue) favor bigoptim." width="384"><p class="caption">
Speed in microseconds of glmnet and bigoptim runs. Positive values (blue) favor bigoptim.
</p>
</div>
<p>In terms of loss, however, we see that <strong>bigoptim</strong> suffers when there are many features in the model, particularly when the number of observations are relatively low.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(data_medium_diff<span class="op">$</span>loss))
lattice<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/lattice/topics/levelplot">levelplot</a></span>(
  loss <span class="op">~</span><span class="st"> </span>cols<span class="op">*</span>rows,
  <span class="dt">data =</span> data_medium_diff,
  <span class="dt">asp =</span> <span class="dv">1</span>,
  <span class="dt">at =</span> <span class="kw">seq</span>(<span class="op">-</span>z, z, <span class="dt">length.out =</span> <span class="dv">20</span>),
  <span class="dt">col.regions =</span> gradpal,
  <span class="dt">main =</span> <span class="st">"Loss (glmnet - bigoptim)"</span>
)</code></pre></div>
<div class="figure">
<img src="test-results_files/figure-html/unnamed-chunk-2-1.png" alt="Difference in objective function loss for glmnet and bigoptim. Negative (red) values favor glmnet." width="384"><p class="caption">
Difference in objective function loss for glmnet and bigoptim. Negative (red) values favor glmnet.
</p>
</div>
</div>
<div id="medium-hard" class="section level2">
<h2 class="hasAnchor">
<a href="#medium-hard" class="anchor"></a>Medium-hard</h2>
<p>This section is similar to the previous one but relates to L1-regularization, rather than L2-regularization and instead compares <strong>glmnet</strong> with the logistic regression fitter of the python <strong>scikit-learn</strong> module. In particular, we will look at the SAGA algorithm. As before, we will use the <code>bigoptim::covtype.svm</code> data set and fit models to feature matrices of increasing rows and columns.</p>
<p>The R package <strong>reticulate</strong> is used to interface with python in the data simulations, which, as before, are featured at <a href="https://github.com/jolars/gsoc18saga/blob/master/data-raw/benchmarks.R" class="uri">https://github.com/jolars/gsoc18saga/blob/master/data-raw/benchmarks.R</a>. The dataset is made part of this package and available as <code><a href="http://www.rdocumentation.org/packages/gsoc18saga/topics/data_mediumhard">gsoc18saga::data_mediumhard</a></code>.</p>
<p>As in the comparison between <strong>glmnet</strong> and <strong>bigoptim</strong>, we again see the same pattern of decreasing performance for <strong>glmnet</strong> with large datasets.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data_mediumhard_diff &lt;-<span class="st"> </span>data_mediumhard <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/tidyr/topics/gather">gather</a></span>(<span class="st">"var"</span>, <span class="st">"value"</span>, time, loss) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/tidyr/topics/spread">spread</a></span>(package, value) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span>(<span class="dt">diff =</span> glmnet <span class="op">-</span><span class="st"> </span>saga) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/select.html">select</a></span>(<span class="op">-</span>glmnet, <span class="op">-</span>saga) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/tidyr/topics/spread">spread</a></span>(var, diff)

z &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(data_mediumhard_diff<span class="op">$</span>time))

lattice<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/lattice/topics/levelplot">levelplot</a></span>(
  time <span class="op">~</span><span class="st"> </span>cols<span class="op">*</span>rows, 
  <span class="dt">data =</span> data_mediumhard_diff,
  <span class="dt">asp =</span> <span class="dv">1</span>,
  <span class="dt">at =</span> <span class="kw">seq</span>(<span class="op">-</span>z, z, <span class="dt">length.out =</span> <span class="dv">20</span>),
  <span class="dt">col.regions =</span> gradpal,
  <span class="dt">main =</span> <span class="st">"Speed (glmnet - scikit-learn)"</span>
)</code></pre></div>
<div class="figure">
<img src="test-results_files/figure-html/unnamed-chunk-3-1.png" alt="Difference in speed (in microseconds) of glmnet and scikit-learn (SAGA) runs. Positive (blue) values favor scikit-learn." width="384"><p class="caption">
Difference in speed (in microseconds) of glmnet and scikit-learn (SAGA) runs. Positive (blue) values favor scikit-learn.
</p>
</div>
<p>Unlike in the previous section, however, the objective function values remain comparable between <strong>glmnet</strong> and <strong>scikit-learn</strong> for most dataset sizes. There is indeed an advantage for <strong>glmnet</strong> but it really only shows for the smallest feature matrix that we assessed with 100 rows and 2 columns.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(data_mediumhard_diff<span class="op">$</span>loss))

lattice<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/lattice/topics/levelplot">levelplot</a></span>(
  loss <span class="op">~</span><span class="st"> </span>cols<span class="op">*</span>rows, 
  <span class="dt">data =</span> data_mediumhard_diff,
  <span class="dt">asp =</span> <span class="dv">1</span>,
  <span class="dt">at =</span> <span class="kw">seq</span>(<span class="op">-</span>z, z, <span class="dt">length.out =</span> <span class="dv">20</span>),
  <span class="dt">col.regions =</span> gradpal,
  <span class="dt">main =</span> <span class="st">"Loss (glmnet - scikit-learn)"</span>
)</code></pre></div>
<div class="figure">
<img src="test-results_files/figure-html/unnamed-chunk-4-1.png" alt="Difference in objective function loss between glmnet and bigoptim. Negative (red) values favor glmnet." width="384"><p class="caption">
Difference in objective function loss between glmnet and bigoptim. Negative (red) values favor glmnet.
</p>
</div>
<p>Taken together, the results suggest that there <em>is</em> benefit to be gained from porting the SAGA algorithm to R. The benefit is perhaps even greater than demonstrated here, given that the simulations here carry some overhead from having called python via R.</p>
</div>
<div id="hard" class="section level2">
<h2 class="hasAnchor">
<a href="#hard" class="anchor"></a>Hard</h2>
<p>The solution to the last test is presented in the package that this vignette is part of. I have written the functions <code><a href="../reference/objective_r.html">objective_r()</a></code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">objective_r &lt;-<span class="st"> </span><span class="cf">function</span>(beta0, beta, x, y, lambda, <span class="dt">alpha =</span> <span class="dv">0</span>) {
  n &lt;-<span class="st"> </span><span class="kw">length</span>(y)
  <span class="co"># binomial loglikelihood</span>
  z &lt;-<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span><span class="kw">crossprod</span>(x, beta)
  loglik &lt;-<span class="st"> </span><span class="kw">sum</span>(y<span class="op">*</span>z <span class="op">-</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(z)))

  <span class="co"># compute penalty</span>
  penalty &lt;-<span class="st"> </span><span class="fl">0.5</span><span class="op">*</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>alpha)<span class="op">*</span><span class="kw">sum</span>(beta<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>alpha<span class="op">*</span><span class="kw">sum</span>(<span class="kw">abs</span>(beta))
  <span class="op">-</span>loglik<span class="op">/</span>n <span class="op">+</span><span class="st"> </span>lambda<span class="op">*</span>penalty
}</code></pre></div>
<p>and <code><a href="../reference/objective_cpp.html">objective_cpp()</a></code>:</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="pp">#include </span><span class="im">&lt;RcppArmadillo.h&gt;</span>

<span class="co">// [[Rcpp::export]]</span>
<span class="dt">double</span> objective_cpp(<span class="dt">double</span> beta0,
                     arma::vec beta,
                     arma::mat x,
                     arma::uvec y,
                     <span class="dt">double</span> lambda,
                     <span class="dt">double</span> alpha = <span class="dv">0</span>) {
  <span class="dt">int</span> n = y.n_elem;
  arma::mat z = beta0 + x.t()*beta;
  <span class="dt">double</span> loglik = arma::accu(y%z - arma::log(<span class="dv">1</span> + arma::exp(z)));
  <span class="dt">double</span> penalty = <span class="fl">0.5</span>*(<span class="dv">1</span> - alpha)*arma::accu(arma::square(beta)) +
    alpha*arma::accu(arma::abs(beta));
  <span class="cf">return</span> -loglik/n + lambda*penalty;
}</code></pre></div>
<p>The concurrence of the two functions is examplified through the following lines, which have been added as a unit test to the package, the result of which you can view <a href="https://travis-ci.org/jolars/gsoc18saga">here</a>, as required by the GSoC test.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit a L1-regularized logistic regression</span>
x &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">100</span><span class="op">*</span><span class="dv">20</span>), <span class="dv">100</span>, <span class="dv">20</span>)
y &lt;-<span class="st"> </span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/sample.html">sample</a></span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="dv">100</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>)
alpha &lt;-<span class="st"> </span><span class="dv">1</span>
fit &lt;-<span class="st"> </span>glmnet<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/glmnet/topics/cv.glmnet">cv.glmnet</a></span>(x, y, <span class="dt">family =</span> <span class="st">"binomial"</span>, <span class="dt">alpha =</span> alpha)

<span class="co"># collect parameters</span>
lambda &lt;-<span class="st"> </span>fit<span class="op">$</span>lambda.1se
beta0 &lt;-<span class="st"> </span><span class="kw">coef</span>(fit, lambda)[<span class="dv">1</span>]
beta &lt;-<span class="st"> </span><span class="kw">coef</span>(fit, lambda)[<span class="op">-</span><span class="dv">1</span>]

<span class="co"># compute objective function values</span>
obj_r &lt;-<span class="st"> </span><span class="kw"><a href="../reference/objective_r.html">objective_r</a></span>(beta0, beta, <span class="kw">t</span>(x), y, lambda, <span class="dt">alpha =</span> alpha)
obj_cpp &lt;-<span class="st"> </span><span class="kw"><a href="../reference/objective_cpp.html">objective_cpp</a></span>(beta0, beta, <span class="kw">t</span>(x), y, lambda, <span class="dt">alpha =</span> alpha)

<span class="co"># check for equality</span>
<span class="kw">all.equal</span>(obj_r, obj_cpp)
<span class="co">#&gt; [1] TRUE</span></code></pre></div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#easy">Easy</a></li>
      <li><a href="#medium">Medium</a></li>
      <li><a href="#medium-hard">Medium-hard</a></li>
      <li><a href="#hard">Hard</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Johan Larsson.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
