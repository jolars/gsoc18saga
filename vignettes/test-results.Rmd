---
title: "GSoC 2018 SAGA project test results"
author: "Johan Larsson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{GSoC 2018 SAGA project test results}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  message = FALSE,
  warnings = FALSE,
  comment = "#>",
  fig.width = 4,
  fig.height = 3.7,
  dev.args = list(pointsize = 8)
)
library(lattice)
trellis.par.set(fontsize = list(text = 8, points = 4))
```

## Easy

Our first task is to fit a L1-regularized linear model to the spam data set
from **ElemStatLearn** and analyze the results in terms of the selected
features as well as test error and AUC. We will also compare our model to a
naive model that predicts the most frequent class.

We begin by loading our libraries.

```{r libraries, message = FALSE}
library(gsoc18saga)
library(glmnet)
library(ElemStatLearn)
```

We will use the `ElemStatLearn::spam` dataset to try to classify email
as either spam or regular email. We load the data and set up balanced 
train and test partitions using the **caret** package.

```{r train}
# extract the necessary data
x <- as.matrix(spam[, -ncol(spam)])
y <- spam$spam
n <- nrow(x)

# create train and test sets
train_id <- caret::createDataPartition(y, p = 0.75)[[1L]]
train_x <- x[train_id, ]
train_y <- y[train_id]
test_x <- x[-train_id, ]
test_y <- y[-train_id]

# train the model
fit <- glmnet::cv.glmnet(train_x, train_y, family = "binomial")
```

The model's chosen features are

```{r features}
names(coef(fit)[coef(fit)[, 1] > 0, ])
```

Next we'll examine the performance of the model using
Receiver Operating Characteristics (ROC), Area Under the Curve (AUC), and
test error. We'll compare the model against a
naive classification scheme wherein we predict each observation (email) 
to be the most prevalent category of the training set, namely.
``r names(which.max(table(train_y)))``. 

```{r roc, fig.cap = "Receiver operating characteristic curves for the lasso model."}
library(pROC)

roc_glmnet <- roc(test_y,
                  as.vector(predict(fit, test_x, type = "response")),
                  ci = TRUE)
roc_naive <- roc(test_y, rep(1, length(test_y)), ci = TRUE)

plot(roc_glmnet, print.auc = TRUE)
```

The 95% confidence level for the AUC value for our
lasso model is $[`r roc_glmnet$ci[1]`, `r roc_glmnet$ci[3]`]$. We can compare
the glmnet model to the naive model using DeLong's test

```{r delong}
roc.test(roc_glmnet, roc_naive)
```

which unsurprisingly shows that the glmnet model is significantly better. 
Looking at the accuracy of the model, the naive model has an accuracy of 
`r sum(test_y == "email")/length(test_y)` whilst the lasso model
manages an accuracy of
`r sum(predict(fit, test_x, type = "class") == test_y)/length(test_y)`.

## Medium

In this part we are going to compare the performance of **glmnet** with
that of **bigoptim** for L2-regularized model fits. For this purpose,
we are going to use the `bigoptim::covtype.svm` data set and
and see how timings differ depending on the numbers of columns and rows we
use from the data. We will also study objective function values in the
same fashion.

The results for these simulations have been precomputed through code
that is available at <https://github.com/jolars/gsoc18saga/blob/master/data-raw/benchmarks.R>. 
The dataset is made part of this package and available as `gsoc18saga::data_medium`.

Looking at the results from the simulation, we note that `glmnet()` and
`sag_fit()` (bigoptim) perform on par with one another
when either the rows or columns are relatively few.
As we add more observations (rows) and covariates (columns), however,
the SAG algorithm from **bigoptim** outperforms `glmnet()` in speed.

```{r, fig.cap = "Speed in microseconds of glmnet and bigoptim runs. Positive values (blue) favor bigoptim."}
gradpal <- colorRampPalette(RColorBrewer::brewer.pal(11, "RdBu"))(100)

library(tidyr)
library(dplyr)

data_medium_diff <- data_medium %>%
  gather("var", "value", time, loss) %>%
  spread(package, value) %>%
  mutate(diff = glmnet - sag) %>%
  select(-glmnet, -sag) %>%
  spread(var, diff)

z <- max(abs(data_medium_diff$time))

lattice::levelplot(
  time ~ cols*rows, 
  data = data_medium_diff,
  asp = 1,
  at = seq(-z, z, length.out = 20),
  col.regions = gradpal,
  main = "Speed (glmnet - bigoptim)"
)
```

In terms of loss, however, we see that **bigoptim** suffers when
there are many features in the model, particularly when the number of 
observations are relatively low.

```{r, fig.cap = "Difference in objective function loss for glmnet and bigoptim. Negative (red) values favor glmnet."}
z <- max(abs(data_medium_diff$loss))
lattice::levelplot(
  loss ~ cols*rows,
  data = data_medium_diff,
  asp = 1,
  at = seq(-z, z, length.out = 20),
  col.regions = gradpal,
  main = "Loss (glmnet - bigoptim)"
)
```


## Medium-hard

This section is similar to the previous one but relates to 
L1-regularization, rather than L2-regularization and instead compares 
**glmnet** with the logistic regression fitter of the python **scikit-learn**
module. In particular, we will look at the SAGA algorithm. As before,
we will use the `bigoptim::covtype.svm` data set and fit models to
feature matrices of increasing rows and columns. 

The R package **reticulate** is used to interface with python in the 
data simulations, which, as before, are featured at 
<https://github.com/jolars/gsoc18saga/blob/master/data-raw/benchmarks.R>.
The dataset is made part of this package and available as `gsoc18saga::data_mediumhard`.

As in the comparison between **glmnet** and **bigoptim**, we again see
the same pattern of decreasing performance for **glmnet** with
large datasets.
 
```{r, fig.cap = "Difference in speed (in microseconds) of glmnet and scikit-learn (SAGA) runs. Positive (blue) values favor scikit-learn."}
data_mediumhard_diff <- data_mediumhard %>%
  gather("var", "value", time, loss) %>%
  spread(package, value) %>%
  mutate(diff = glmnet - saga) %>%
  select(-glmnet, -saga) %>%
  spread(var, diff)

z <- max(abs(data_mediumhard_diff$time))

lattice::levelplot(
  time ~ cols*rows, 
  data = data_mediumhard_diff,
  asp = 1,
  at = seq(-z, z, length.out = 20),
  col.regions = gradpal,
  main = "Speed (glmnet - scikit-learn)"
)
```

Unlike in the previous section, however, the objective function values remain
comparable between **glmnet** and **scikit-learn** for most dataset sizes.
There is indeed an advantage for **glmnet** but it really only shows
for the smallest feature matrix that we assessed with
`r min(data_mediumhard$rows)`
rows and `r min(data_mediumhard$cols)` columns.

```{r, fig.cap = "Difference in objective function loss between glmnet and bigoptim. Negative (red) values favor glmnet."}
z <- max(abs(data_mediumhard_diff$loss))

lattice::levelplot(
  loss ~ cols*rows, 
  data = data_mediumhard_diff,
  asp = 1,
  at = seq(-z, z, length.out = 20),
  col.regions = gradpal,
  main = "Loss (glmnet - scikit-learn)"
)
```

Taken together, the results suggest that there *is* benefit to be gained from
porting the SAGA algorithm to R. The benefit is perhaps even greater than
demonstrated here, given that the simulations here carry some overhead 
from having called python via R.

## Hard

The solution to the last test is presented in the package that
this vignette is part of. I have written the functions `objective_r()`:

```{r r-objective, eval = FALSE}
objective_r <- function(beta0, beta, x, y, lambda, alpha = 0) {
  n <- length(y)
  # binomial loglikelihood
  z <- beta0 + crossprod(x, beta)
  loglik <- sum(y*z - log(1 + exp(z)))

  # compute penalty
  penalty <- 0.5*(1 - alpha)*sum(beta^2) + alpha*sum(abs(beta))
  -loglik/n + lambda*penalty
}
```

and `objective_cpp()`: 

```{Rcpp cpp-objective, eval = FALSE}
#include <RcppArmadillo.h>

// [[Rcpp::export]]
double objective_cpp(double beta0,
                     arma::vec beta,
                     arma::mat x,
                     arma::uvec y,
                     double lambda,
                     double alpha = 0) {
  int n = y.n_elem;
  arma::mat z = beta0 + x.t()*beta;
  double loglik = arma::accu(y%z - arma::log(1 + arma::exp(z)));
  double penalty = 0.5*(1 - alpha)*arma::accu(arma::square(beta)) +
    alpha*arma::accu(arma::abs(beta));
  return -loglik/n + lambda*penalty;
}
```

The concurrence of the two functions is examplified through the following lines,
which have been added as a unit test to the package, the result of which you
can view [here](https://travis-ci.org/jolars/gsoc18saga), as
required by the GSoC test.

```{r test}
# fit a L1-regularized logistic regression
x <- matrix(rnorm(100*20), 100, 20)
y <- sample(1:2, 100, replace = TRUE)
alpha <- 1
fit <- glmnet::cv.glmnet(x, y, family = "binomial", alpha = alpha)

# collect parameters
lambda <- fit$lambda.1se
beta0 <- coef(fit, lambda)[1]
beta <- coef(fit, lambda)[-1]

# compute objective function values
obj_r <- objective_r(beta0, beta, t(x), y, lambda, alpha = alpha)
obj_cpp <- objective_cpp(beta0, beta, t(x), y, lambda, alpha = alpha)

# check for equality
all.equal(obj_r, obj_cpp)
```

