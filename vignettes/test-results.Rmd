---
title: "GSoC 2018 SAGA project test results"
author: "Johan Larsson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{GSoC 2018 SAGA project test results}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dev.args = list(pointsize = 8)
)
library(lattice)
trellis.par.set(fontsize = list(text = 8, points = 4))
```

## Easy

Our first task is to fit a L1-regularized linear model to the spam data set
from **ElemStatLearn** and analyze the results in terms of the selected
features as well as test error and AUC. We will also compare our model to a
naive model that predicts the most frequent class.

We begin by loading our libraries.

```{r libraries}
library(gsoc18saga)
library(glmnet)
library(ElemStatLearn)
```

Next, we set up train and test partitions using the **caret** package.

```{r train}
# extract the necessary data
x <- as.matrix(spam[, -ncol(spam)])
y <- spam$spam
n <- nrow(x)

# create train and test sets
train_id <- caret::createDataPartition(y, p = 0.75)[[1L]]
train_x <- x[train_id, ]
train_y <- y[train_id]
test_x <- x[-train_id, ]
test_y <- y[-train_id]

# train the model
fit1 <- glmnet::cv.glmnet(train_x, train_y, family = "binomial")
```

The model's chosen factors are

```{r features}
names(coef(fit1)[coef(fit1)[, 1] > 0, ])
```

Now we will study the performance of the model. We are going to use
Receiver Operating Characteristics (ROC) of the model as well as a
naive classification scheme wherein each observation is classified
as the most prevalent category in the training set, namely
``r names(which.max(table(train_y)))``, which is equivalent to an Area Under
the Curve (AUC) of 0.5.

```{r roc, fig.cap = "Receiver operating characteristic curves for the lasso model.", fig.width = 4, fig.height = 4.3}
library(pROC)

roc <- roc(test_y, as.vector(predict(fit1, test_x, type = "response")),
           ci = TRUE)
plot(roc, print.auc = TRUE)
```

The 95% confidence level for the AUC value for our
lasso model is $[`r roc1$ci[1]`, `r roc1$ci[3]`]$, which
is signficantly better than that of our naive predictions.

## Medium

In this part we are going to compare the performance of **glmnet** with
that of **bigoptim** using **microbenchmark**. For this purpose,
we are going to use the `covtype.libsvm` dataset from **bigoptim**
and see how timings differ depending on the numbers of columns and rows we
use from the data. We will also study objective function values in the
same fashion.

Looking at the results from the simulation, we note that `glmnet()` and
`sag_fit()` (bigoptim) perform on par with one antother
when either the number of rows or columns are relatively low.
As we add more observations (rows) and covariates (columns), however,
the SAG algorithm from **bigoptim** outperforms `glmnet()`.

```{r, fig.cap = "Speed of glmnet and bigoptim runs. Colors indicate execution time in logarithmized nanoseconds."}
gradpal <- colorRampPalette(RColorBrewer::brewer.pal(11, "RdBu"))(100)

data_medium_diff <- data_medium %>%
  gather("var", "value", time, loss) %>%
  spread(package, value) %>%
  mutate(diff = glmnet - sag) %>%
  select(-glmnet, -sag) %>%
  spread(var, diff)

z <- max(abs(data_medium_diff$time))

lattice::levelplot(
  time ~ cols*rows, 
  data = data_medium_diff,
  asp = 1,
  at = seq(-z, z, length.out = 20),
  col.regions = gradpal
)
```

```{r, fig.cap = "Difference in objective function loss for glmnet and bigoptim. Negative (red) values favor glmnet."}
z <- max(abs(data_medium_diff$loss))
lattice::levelplot(
  loss ~ cols*rows,
  data = data_medium_diff,
  asp = 1,
  at = seq(-z, z, length.out = 20),
  col.regions = gradpal
)
```


## Medium-hard
 
```{r, fig.cap = "Difference in speed (in nanoseconds) of glmnet and scikit-learn (SAGA) runs. Negative (red) values favor glmnet."}
data_mediumhard_diff <- data_mediumhard %>%
  gather("var", "value", time, loss) %>%
  spread(package, value) %>%
  mutate(diff = glmnet - saga) %>%
  select(-glmnet, -saga) %>%
  spread(var, diff)

z <- max(abs(data_mediumhard_diff$time))

lattice::levelplot(
  time ~ cols*rows, 
  data = data_mediumhard_diff,
  asp = 1,
  at = seq(-z, z, length.out = 20),
  col.regions = gradpal
)
```

```{r, fig.cap = "Difference in objective function loss between glmnet and bigoptim. Colors indicate objective function loss."}
z <- max(abs(data_mediumhard_diff$loss))

lattice::levelplot(
  loss ~ cols*rows, 
  data = data_mediumhard_diff,
  asp = 1,
  at = seq(-z, z, length.out = 20),
  col.regions = gradpal
)
```

## Hard

The solution to the last test is presented in the package that
this vignette is part of. I have written the functions `objective_r()`:

```{r r-objective, eval = FALSE}
objective_r <- function(beta0, beta, x, y, lambda, alpha = 0) {
  n <- length(y)
  # binomial loglikelihood
  z <- beta0 + crossprod(x, beta)
  loglik <- sum(y*z - log(1 + exp(z)))

  # compute penalty
  penalty <- 0.5*(1 - alpha)*sum(beta^2) + alpha*sum(abs(beta))
  -loglik/n + lambda*penalty
}
```

and `objective_cpp()`: 

```{Rcpp cpp-objective, eval = FALSE}
#include <RcppArmadillo.h>

// [[Rcpp::export]]
double objective_cpp(double beta0,
                     arma::vec beta,
                     arma::mat x,
                     arma::uvec y,
                     double lambda,
                     double alpha = 0) {
  int n = y.n_elem;
  arma::mat z = beta0 + x.t()*beta;
  double loglik = arma::accu(y%z - arma::log(1 + arma::exp(z)));
  double penalty = 0.5*(1 - alpha)*arma::accu(arma::square(beta)) +
    alpha*arma::accu(arma::abs(beta));
  return -loglik/n + lambda*penalty;
}
```

The concurrence of the two functions is examplified through the following lines
and is added as a unit test to the package, the result of which you
can view [here](https://travis-ci.org/jolars/gsoc18saga).

```{r test}
# fit a L1-regularized logistic regression
x <- matrix(rnorm(100*20), 100, 20)
y <- sample(1:2, 100, replace = TRUE)
alpha <- 1
fit <- glmnet::cv.glmnet(x, y, family = "binomial", alpha = alpha)

# collect parameters
lambda <- fit$lambda.1se
beta0 <- coef(fit, lambda)[1]
beta <- coef(fit, lambda)[-1]

# compute objective function values
obj_r <- objective_r(beta0, beta, t(x), y, lambda, alpha = alpha)
obj_cpp <- objective_cpp(beta0, beta, t(x), y, lambda, alpha = alpha)

# check for equality
all.equal(obj_r, obj_cpp)
```

